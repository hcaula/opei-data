{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities A and B\n",
    "The A and B modalities are very similar in the way they're structured, so it makes sense to treat them similarly. For the Modality C, some few adjustments are going to have to be made, so we'll deal with it later.\n",
    "\n",
    "### Objectives\n",
    "- Retrieve the amount of right answers per question\n",
    "- Retrieve the amount of candidates for each score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full results CSV's\n",
    "mod_a_results = pd.read_csv('./data/mod_a.csv', names=['code', 'score'], index_col=False)\n",
    "mod_b_results = pd.read_csv('./data/mod_b.csv', names=['code', 'score'], index_col=False)\n",
    "\n",
    "# Number of canditades for each modality\n",
    "total_mod_A = len(mod_a_results.index)\n",
    "total_mod_B = len(mod_b_results.index)\n",
    "\n",
    "total = {'A': mod_a_results, 'B': mod_b_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading individual rooms results\n",
    "files = {\n",
    "    'A': [\n",
    "        open('./data/mod_a_sala_4.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_a_sala_5.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_a_sala_6.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_a_sala_7.txt', encoding='utf-8').read()\n",
    "    ],\n",
    "    'B': [\n",
    "        open('./data/mod_b_sala_11.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_b_sala_12.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_b_sala_13.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_b_sala_14.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_b_sala_15.txt', encoding='utf-8').read(),\n",
    "        open('./data/mod_b_extra.txt', encoding='utf-8').read()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regex to retrieve only the answer list\n",
    "\n",
    "answers = {'A': [], 'B': []}\n",
    "for modality in files:\n",
    "    for room in files[modality]:\n",
    "        \n",
    "        # Removing break lines and whitespaces for easier regex pattern\n",
    "        string = room.replace('\\n', '').replace(' ','');\n",
    "\n",
    "        # .*? - Anything until you find the next pattern\n",
    "        # \\d* - Any combination of digits (e.g.: 1, 5, 20), indicating the question number\n",
    "        # \\.  - The dot character between the question number and the option marked\n",
    "        # \\w  - Any character, indicating the option that the student marked\n",
    "        # \\+* - The plus signal, indicating if it's the correct answer (asterisk because it might not be a right answer)\n",
    "        matches = re.findall('.*?(\\d*\\.\\w\\+*)', string)\n",
    "\n",
    "        # extend() instead of append() in order to add each individual element to list\n",
    "        answers[modality].extend(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which answers are right ones based on weather or not  it contains a plus sign besides it\n",
    "\n",
    "right_answers = {'A': [], 'B': []}\n",
    "for modality in answers:\n",
    "    for answer in answers[modality]:\n",
    "        \n",
    "        # Matches two groups: the question number and the plus sign\n",
    "        matches = re.match('(\\d*)\\.\\w(\\+)*', answer)\n",
    "        number = matches.group(1)\n",
    "        is_right = matches.group(2)\n",
    "\n",
    "        # Checking if answer list is over and it's a right question\n",
    "        if (number and is_right):\n",
    "            question_number = int(number)\n",
    "\n",
    "            # Appends 0 to the list if it hasn't already\n",
    "            if (len(right_answers[modality]) < question_number): right_answers[modality].append(1)\n",
    "            else: right_answers[modality][question_number-1] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing Data Frame, adding some new columns and exporting to CSV format\n",
    "\n",
    "questions = pd.DataFrame(right_answers)\n",
    "questions = questions.set_index([list(range(1,21))])\n",
    "questions.index.name = 'Question'\n",
    "questions['%A'] = questions.apply(lambda x: ((x[0]/total_mod_A)*100).round(1), axis=1)\n",
    "questions['%B'] = questions.apply(lambda x: ((x[1]/total_mod_B)*100).round(1), axis=1)\n",
    "questions['normalized_A'] = questions.apply(lambda x: (x[0]/max(questions['A'])).round(1), axis=1)\n",
    "questions['normalized_B'] = questions.apply(lambda x: (x[1]/max(questions['B'])).round(1), axis=1)\n",
    "\n",
    "mod_a = questions[['A', '%A', 'normalized_A']].copy()\n",
    "mod_a = mod_a.rename(columns = {'A': 'Total', '%A': '%', 'normalized_A': 'Normalized'})\n",
    "\n",
    "mod_b = questions[['B', '%B', 'normalized_B']].copy()\n",
    "mod_b = mod_b.rename(columns = {'B': 'Total', '%B': '%', 'normalized_B': 'Normalized'})\n",
    "\n",
    "mod_a.to_csv('./outputs/questions_mod_a.csv')\n",
    "mod_b.to_csv('./outputs/questions_mod_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the scores DataFrame\n",
    "\n",
    "scores = {'A': [], 'B': []}\n",
    "for i in list(range(21)):\n",
    "    score_measure_mod_a = len(mod_a_results[mod_a_results.score == i].index)\n",
    "    score_measure_mod_b = len(mod_b_results[mod_b_results.score == i].index)\n",
    "    \n",
    "    scores['A'].append(score_measure_mod_a)\n",
    "    scores['B'].append(score_measure_mod_b)\n",
    "    \n",
    "scores = pd.DataFrame(scores).set_index([list(range(0,21))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.index.name = 'Score'\n",
    "scores['%A'] = scores.apply(lambda x: ((x[0]/total_mod_A)*100).round(2), axis=1)\n",
    "scores['%B'] = scores.apply(lambda x: ((x[1]/total_mod_B)*100).round(2), axis=1)\n",
    "scores['normalized_A'] = scores.apply(lambda x: (x[0]/max(scores['A'])).round(2), axis=1)\n",
    "scores['normalized_B'] = scores.apply(lambda x: (x[1]/max(scores['B'])).round(2), axis=1)\n",
    "\n",
    "mod_a = scores[['A', '%A', 'normalized_A']].copy()\n",
    "mod_a = mod_a.rename(columns = {'A': 'Total', '%A': '%', 'normalized_A': 'Normalized'})\n",
    "\n",
    "mod_b = scores[['B', '%B', 'normalized_B']].copy()\n",
    "mod_b = mod_b.rename(columns = {'B': 'Total', '%B': '%', 'normalized_B': 'Normalized'})\n",
    "\n",
    "mod_a.to_csv('./outputs/scores_mod_a.csv')\n",
    "mod_b.to_csv('./outputs/scores_mod_b.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modality C\n",
    "The Modality C exam consisted of marathon-like questions, therefore, should be treated diferently.\n",
    "\n",
    "### Objectives\n",
    "- Retrieve the number of right answers per question\n",
    "- Retrieve the number of attempts for each output (wrong answer, TLE)\n",
    "- Retrieve the number of candidates per score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of candidates per score\n",
    "mod_c_results = pd.read_csv('./data/mod_c.csv', names=['name', 'score', 'normalized'])\n",
    "\n",
    "# Deleting the names for protection\n",
    "del mod_c_results['name']\n",
    "\n",
    "results = []\n",
    "for grade in list(range(0, 9)):\n",
    "    cand_per_grade = len(mod_c_results[(mod_c_results.score > grade) & (mod_c_results.score <= (grade+1))])\n",
    "    results.append(cand_per_grade)\n",
    "\n",
    "pd.DataFrame(results).to_csv('./outputs/scores_mod_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = pd.read_csv('./data/mod_c_g1.csv')\n",
    "g2 = pd.read_csv('./data/mod_c_g1.csv')\n",
    "g3 = pd.read_csv('./data/mod_c_g1.csv')\n",
    "outputs_mod_c = pd.concat([g1, g2, g3], ignore_index=True)\n",
    "del outputs_mod_c['StudentName']\n",
    "del outputs_mod_c['StudentEmail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of attempts for each output\n",
    "total_outputs = {}\n",
    "\n",
    "for output in outputs_mod_c.SubmissionEvaluation:\n",
    "    if (not output in total_outputs): total_outputs[output] = 1\n",
    "    else: total_outputs[output] += 1\n",
    "\n",
    "df = pd.DataFrame(data=total_outputs, index=[0]).transpose()\n",
    "scores.index.name = 'Outputs'\n",
    "df.to_csv('./outputs/outputs_mod_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
